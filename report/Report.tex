\documentclass[11pt, oneside]{article}
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{xcolor,sectsty}
\usepackage{float}
\usepackage{authblk}

\bibliographystyle{apalike}

% One must be careful when importing hyperref. Usually it has to be the last package to be imported, but there might be some exceptions to this rule.
\usepackage{hyperref}

% Styles
\definecolor{astral}{RGB}{46,116,181}
% \subsectionfont{\color{astral}}
% \sectionfont{\color{astral}}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=violet
}
 
\urlstyle{same}
%SetFonts

%SetFonts


\title{Sentiment Analysis using Word Vectors}
\author{ATJ}
\author{NSB}
\affil{University of Massachussets, Amherst}
\date{}
\begin{document}
\maketitle
\pagenumbering{gobble}
\newpage
\pagenumbering{arabic}
\begin{abstract}
For the task of sentiment analysis, a document can be represented in multiple forms - as a bag of words of unigram frequencies, as a bag of words of unigram absence/presence indicators, as a bag of words of bigram frequencies, or as word vectors generated from Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), word2vec or doc2vec. The selection of a particular document representation is crucial to yield good accuracies in sentiment analysis. In this project we evaluate these different document representations over three widely available datasets - Review Polarity v2.0 dataset and the Subjectivity dataset provided by \cite{pang2004sentimental}, and the Large Movie Review Dataset v1.0 made available by \cite{maas2011learning} using a multitude of classifiers - Naïve Bayes, Logistic Regression, K-Nearest Neighbors, Decision Trees, Support Vector Machines and Ensemble classifiers such as AdaBoost, Bagging and Random Forest. The results we obtained align with those obtained by \cite{maas2011learning}. We find that word vector representations using word2vec and doc2vec are the best document representations for sentiment analysis as they capture both semantic and sentiment information. 
\end{abstract}
\section{Introduction}
\paragraph{}
Sentiment Analysis is the task of extracting the favorable or unfavorable inclination of a person towards a subject or topic. For example, we could analyze twitter feeds to predict or assess the stock market trends, the favorability of a presidential nominee during elections, or how good a movie is based on the volume of the tweets and their content. It can be used to get user opinions on the products the user purchased based on the star ratings and reviews submitted through online portals like amazon.com which can be used by product manufacturers to identify how well their product is received by the majority of users. This kind of feedback is invaluable and very crucial in many cases. \textit{How can we represent a document so that we achieve high accuracies in sentiment analyis? What information would this representation capture?}
\paragraph{}
For the purpose of sentiment analysis, one could view a document as simply a ``bag of words'' in which the position of words does not matter. In addition, we could assume that the words are conditionally independent given the class to which they belong. This is implicitly errant because it ignores the possibility of negated reviews like negated positive reviews which become negative and negated negative which might become positive. In addition, it ignores the semantics of the words. For example, it cannot capture the relationship between ``powerful'', ``strong'' and ``Paris''. Vector based models are able to capture the relational structure of the lexicon. Vector based models can represent words as distance or angle between word vectors in a high-dimensional space. This gives us ability to evaluate word similarities. Different word vectors capture different information about the documents. Some word vectors capture semantic information of the words, while other representations capture both semantic and sentiment information. The particular document representation selected for sentiment analysis has a huge impact on the accuracy of the classifier.
\paragraph{}
In this project, we evaluate some of these different document representations - bag of words using unigram frequencies, bag of words using unigram absenece/presence indicators, bag of words using bigrams, word vectors obtained from Latent Dirichlet Allocation (LDA) \cite{blei2003latent}, Latent Semantic Allocation (LSA), word2vec \cite{le2014distributed} and doc2vec using a multitude of classifiers such as Naïve Bayes, Logistic Regression, K-Nearest Neighbors, Decision Trees, Random Forests, Support Vector Machines and Ensemble classifiers. We evaluate these different document representations by training and testing the classifiers over three widely available datasets - Review Polarity v2.0 dataset and the Subjectivity dataset provided by \cite{pang2004sentimental}, and the Large Movie Review Dataset v1.0 made available by \cite{maas2011learning}. The accuracies that \cite{maas2011learning} and others \cite{sadeghianbag} reported on the three datasets are listed in Table \ref{table:1}. Our experiments similarly reveal that we can achieve high accuracies in sentiment analysis using word vector representations obtained from word2vec and doc2vec. This is possible because these word vector representations capture both semantic as well as sentiment information. 

\begin{table}[H]
\begin{tabular}{ | l | p{1.5cm} | p{2.5cm} | p{2.5cm} |  }
 \hline
 \textbf{Features} & \textbf{PL04} & \textbf{IMDB Dataset} & \textbf{Subjectivity} \\
 \hline
 Bag of Words   & 85.45 & 87.80 & 87.77 \\
 LDA & 66.70 & 67.42 & 66.65 \\
 LSA & 84.55 & 83.96 & 82.82 \\
 \cite{maas2011learning}'s Semantic Only & 87.10 & 87.30 & 86.65 \\
 \cite{maas2011learning}'s Full & 84.65 & 87.44 & 86.19 \\
 \cite{maas2011learning}'s Full + Bag of Words (bnc) & 87.85 & 88.33 & 88.45 \\
 Bag of Words SVM \cite{pang2004sentimental} & 87.15 & N/A & 90.00 \\
 \hline
\end{tabular}
\caption{Classification accuracies reported by \cite{maas2011learning} and others for different word representations.}
\label{table:1}
\end{table}
\section{Related Work}
explain what other approaches have been to the problem. Cite specific instances of previous work. You must cite at least 10 relevant research papers, and describe them and how they relate to your work. It may be convenient to structure this as a related work or literature review section.
\section{Data}
\paragraph{}
We employ the Review Polarity v2.0 dataset and the Subjectivity dataset provided by (\cite{pang2004sentimental}) which are available online at \url{http://www.cs.cornell.edu/People/pabo/movie-review-data/}. We also use the \href{http://ai.stanford.edu/~amaas/data/sentiment/}{Large Movie Review Dataset v1.0} made available by \cite{maas2011learning}. 
\paragraph{}
The Review polarity dataset v2.0 consists of 2,000 movie reviews that were obtained from \href{http://reviews.imdb.com/Reviews}{IMDb archives} and processed. The folders ``pos'' or ``neg'' under which these movie review files exist determine the true sentiment label. The subjectivity dataset contains two files, one containing 5000 objective sentences and the other containing 5000 subjective sentences. The subjective sentences were obtained by processing movie reviews from \href{http://www.rottentomatoes.com/}{Rotten Tomatoes} and the objective sentences were obtained by processing plot summaries for movies from \href{http://www.imdb.com}{IMDb}.
\paragraph{} 
The Large Movie Review Dataset v1.0 contains 50,000 movie reviews split evenly into 25k train and 25k test sets. The movie review files are under two folders, ``test'' and ``train'', each containing a ``pos'' and a ``neg'' folder having reviews of the corresponding label. The overall distribution of labels is balanced (25k pos and 25k neg). The train and test sets contain a disjoint set of movies, so no significant performance is obtained by memorizing movie-unique terms and their associated labels.
\section{Method}
We built different pipelines with stages of preprocessing, feature extraction, classifier training, hyper parameter selection and evaluation using python as the programming language and code libraries from sci-kit learn, nltk, gensim.
\subsection{Preprocessing}
We built three tokenizers - Simple Tokenizer, Advanced Tokenizer and Bigram Tokenizer. The Simple Tokenizer simply splits documents on spaces and down cases the tokens. The Advanced Tokenizer is built using nltk's \textit{TreebankWordTokenizer} which tokenizes text as in the Penn Treebank. The \textit{TreebankWordTokenizer} splits standard contractions such as ``don't'' to ``do n't'' and ``they'll'' to ``they 'll'', treats most punctuation characters as separate tokens, splits off commas and single quotes when followed by whitespace and separates periods that appear at the end of line. We also remove stop words using nltk's \textit{stopwords} corpus for the English language. Finally, we down case the tokens. The Bigram Tokenizer uses ntlk's \textit{TreebankWordTokenizer} to tokenize the text. The tokens are then grouped into bigrams using ntlk's \textit{bigram} package.
\subsection{Feature Extraction}
The features that we used in our experiments were unigram frequencies, bigram frequencies, unigram and bigram LSA, unigram and bigram LDA, word2vec and doc2vec.
\paragraph{}
The word2vec model was setup to have a feature size of 100 dimensions, window size of 10, 7 worker processes and sample size of 1e-4. In order to avoid loading all the movie reviews into memory at the same time, we streamed one file at a time required for processing. The word2vec model was trained in 10 epochs, with the reviews shuffled randomly in each epoch. The generated word2vec model was saved to disk to avoid regeneration each time we needed to run an experiment. Vectors for a document were generated by summing up the word vectors for each token in the document.
\paragraph{}
The doc2vec model was setup to have 
\section{Results}
Describe the experiments you ran and identify your baseline method(s). Include the results you achieved with the various methods you are comparing making. This section will probably also include some figures that succinctly summarize your results. Analyze your results (including your models). If you did exploratory analysis or a significant amount of feature engineering, your analysis may merit its own section. After reading this section (and your dataset and methods), an interested reader should be able to duplicate your experiments and results.
\section{Discussion and Future Work}
discuss any implications of your analysis for the problem as a whole, and what are the next steps for future work. Any other concluding remarks should go here.

\bibliography{report_bibliography}
\end{document}